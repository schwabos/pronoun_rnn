{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will extract the data from 315 files which are found across three difference databases:  \n",
    "All files are in xml format  \n",
    "We will use BeautifulSoup to extract the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This is the general structure of the relevant corpora and the fields that will be useful:\n",
    "# ParCor (http://opus.lingfil.uu.se/ParCor/)\n",
    "\n",
    "# <word id=\"word_1\">Social</word>\n",
    "\n",
    "# <markable \n",
    "# id=\"markable_10849\" \n",
    "# span=\"word_30477..word_30480\" position=\"none\"  coref_class=\"empty\"  mmax_level=\"coref\"  \n",
    "# nptype=\"np\"  agreement=\"none\"  \n",
    "# mention=\"np\"  type=\"none\"  sub_apposition=\"none\" />\n",
    "\n",
    "# ACLPro (http://www.aclweb.org/anthology/C12-2103)\n",
    "\n",
    "# <word id=\"word_1\">Social</word>\n",
    "\n",
    "# <markable \n",
    "# id=\"markable_825\" \n",
    "# span=\"word_1438..word_1443\" \n",
    "# coref_class=\"set_114\"  mmax_level=\"coref\"  sure=\"yes\"  \n",
    "# np_form=\"ne\" />\n",
    "\n",
    "\n",
    "# WikiPro (http://rali.iro.umontreal.ca/rali/?q=en/wikicoref)\n",
    "\n",
    "# <word id=\"word_1\">Aberfoyle</word>\n",
    "\n",
    "# <markable \n",
    "# id=\"markable_318\" \n",
    "# span=\"word_1..word_1\" \n",
    "# coref_class=\"set_47\" topic=\"http://rdf.freebase.com/ns/m.010kl\" coreftype=\"ident\" \n",
    "# mentiontype=\"ne\"  mmax_level=\"coref\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lists of all the file names to easily scrape them\n",
    "\n",
    "Two types of files needed, the word files and the annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parcor_EU = !ls Kojak_Databases/ParCorPro/Annotated_Texts/EU_Bookshop/English/Annotator1\n",
    "parcor_TED_words = !ls Kojak_Databases/ParCorPro/Annotated_Texts/TED/English/Annotator1/Basedata\n",
    "parcor_TED_ann = !ls Kojak_Databases/ParCorPro/Annotated_Texts/TED/English/Annotator1/Markables/Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KEBC11002ENE',\n",
       " 'KEBC12001ENE',\n",
       " 'KH7911105ENE',\n",
       " 'MI3112464ENE',\n",
       " 'MJ3011331ENE',\n",
       " 'NA3211776ENE',\n",
       " 'QE3011322ENE',\n",
       " 'QE3211790ENE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcor_EU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parcor_word_files = []\n",
    "parcor_ann_files = []\n",
    "\n",
    "for file in parcor_EU:\n",
    "    word_files = 'Kojak_Databases/ParCorPro/Annotated_Texts/EU_Bookshop/English/Annotator1/'+file+'/Basedata/'+file+'_words.xml'\n",
    "    ann_files = 'Kojak_Databases/ParCorPro/Annotated_Texts/EU_Bookshop/English/Annotator1/'+file+'/Markables/'+file+'_coref_level.xml'\n",
    "    parcor_word_files.append(word_files)\n",
    "    parcor_ann_files.append(ann_files)\n",
    "    \n",
    "for file in parcor_TED_words:\n",
    "    word_files = 'Kojak_Databases/ParCorPro/Annotated_Texts/TED/English/Annotator1/Basedata/'+file\n",
    "    parcor_word_files.append(word_files)\n",
    "    \n",
    "for file in parcor_TED_ann:\n",
    "    ann_files = 'Kojak_Databases/ParCorPro/Annotated_Texts/TED/English/Annotator1/Markables/Coref/'+file\n",
    "    parcor_ann_files.append(ann_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aclpro_C = !ls Kojak_Databases/ACLPro/annotation/C\n",
    "aclpro_D = !ls Kojak_Databases/ACLPro/annotation/D\n",
    "aclpro_P = !ls Kojak_Databases/ACLPro/annotation/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aclpro_word_files = []\n",
    "aclpro_ann_files = []\n",
    "\n",
    "for file in aclpro_C:\n",
    "    word_files = 'Kojak_Databases/ACLPro/annotation/C/'+file+'/Basedata/'+file+'_words.xml'\n",
    "    ann_files = 'Kojak_Databases/ACLPro/annotation/C/'+file+'/Markables/'+file+'_coref_level.xml'\n",
    "    aclpro_word_files.append(word_files)\n",
    "    aclpro_ann_files.append(ann_files)\n",
    "\n",
    "\n",
    "for file in aclpro_D:\n",
    "    word_files = 'Kojak_Databases/ACLPro/annotation/D/'+file+'/Basedata/'+file+'_words.xml'\n",
    "    ann_files = 'Kojak_Databases/ACLPro/annotation/D/'+file+'/Markables/'+file+'_coref_level.xml'\n",
    "    aclpro_word_files.append(word_files)\n",
    "    aclpro_ann_files.append(ann_files)\n",
    "\n",
    "\n",
    "for file in aclpro_P:\n",
    "    word_files = 'Kojak_Databases/ACLPro/annotation/P/'+file+'/Basedata/'+file+'_words.xml'\n",
    "    ann_files = 'Kojak_Databases/ACLPro/annotation/P/'+file+'/Markables/'+file+'_coref_level.xml'\n",
    "    aclpro_word_files.append(word_files)\n",
    "    aclpro_ann_files.append(ann_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikipro = !ls Kojak_Databases/WikiPro/Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipro_word_files = []\n",
    "wikipro_ann_files = []\n",
    "\n",
    "for file in wikipro:\n",
    "    word_files = 'Kojak_Databases/WikiPro/Annotation/'+file+'/Basedata/'+file+'_words.xml'\n",
    "    ann_files = 'Kojak_Databases/WikiPro/Annotation/'+file+'/Markables/'+file+'_coref_level.xml'\n",
    "    wikipro_word_files.append(word_files)\n",
    "    wikipro_ann_files.append(ann_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kojak_Databases/WikiPro/Annotation/Aberfoyle, Stirling/Basedata/Aberfoyle, Stirling_words.xml'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipro_word_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data from all the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parcor_all_words = {}\n",
    "j = 1\n",
    "for file in parcor_word_files:\n",
    "    parcor_all_words[j] = {}\n",
    "    word_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    k = 1\n",
    "    for word in word_list_xml.findAll('word'):\n",
    "        parcor_all_words[j][k] = {'word': word.text,\n",
    "                                  'place': \"word_\" + str(k),\n",
    "                                  'doc':j}\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aclpro_all_words = {}\n",
    "j = 1\n",
    "for file in aclpro_word_files:\n",
    "    aclpro_all_words[j] = {}\n",
    "    word_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    k = 1\n",
    "    for word in word_list_xml.findAll('word'):\n",
    "        aclpro_all_words[j][k] = {'word': word.text,\n",
    "                                  'place': \"word_\" + str(k),\n",
    "                                  'doc':j}\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikipro_all_words = {}\n",
    "j = 1\n",
    "for file in wikipro_word_files:\n",
    "    wikipro_all_words[j] = {}\n",
    "    word_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    k = 1\n",
    "    for word in word_list_xml.findAll('word'):\n",
    "        wikipro_all_words[j][k] = {'word': word.text,\n",
    "                                  'place': \"word_\" + str(k),\n",
    "                                  'doc':j}\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the structure of parcor\n",
    "# # <markable \n",
    "# # id=\"markable_10849\" \n",
    "# # span=\"word_30477..word_30480\" \n",
    "# position=\"none\"  \n",
    "# coref_class=\"empty\" \n",
    "# mmax_level=\"coref\"  \n",
    "# # nptype=\"np\"  \n",
    "# agreement=\"none\"  \n",
    "# # mention=\"np\"  \n",
    "# type=\"none\" \n",
    "# sub_apposition=\"none\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parcor_all_ann = {}\n",
    "j = 1\n",
    "for file in parcor_ann_files:\n",
    "    parcor_all_ann[j] = {}\n",
    "    ann_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    i = 1\n",
    "    for ref in ann_list_xml.findAll('markable'):\n",
    "        this = {\"id\":'',\"span\":'',\"coref_class\":'',\n",
    "                \"mmax_level\":'',\"nptype\":'',\"agreement\":'',\"mention\":'',\"type\":'',\"sub_apposition\":''}\n",
    "\n",
    "        this[\"id\"] = ref['id']\n",
    "        this[\"span\"] = ref['span']\n",
    "        this[\"coref_class\"] = ref['coref_class']\n",
    "        this[\"mmax_level\"] = ref['mmax_level']\n",
    "        this[\"mention\"] = ref['mention']\n",
    "        this['first_span'] = ref['span'].split('..')[0] #I am taking the first word of the span, to have one to one\n",
    "        this['doc'] = j\n",
    "        try:\n",
    "            this[\"nptype\"]= ref['nptype']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            this[\"agreement\"]= ref['agreement']\n",
    "        except:\n",
    "            pass           \n",
    "        try:\n",
    "            this[\"type\"]= ref['type']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            this[\"sub_apposition\"] = ref['sub_apposition']\n",
    "        except:\n",
    "            pass                \n",
    " \n",
    "        parcor_all_ann[j][i] = this\n",
    "        i += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the structure of aclpro\n",
    "# <markable \n",
    "# id=\"markable_67\" \n",
    "# span=\"word_656..word_657\" \n",
    "# coref_class=\"set_3\"  \n",
    "# mmax_level=\"coref\"  \n",
    "# sure=\"yes\"  \n",
    "# np_form=\"def-np\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aclpro_all_ann = {}\n",
    "j = 1\n",
    "for file in aclpro_ann_files:\n",
    "    aclpro_all_ann[j] = {}\n",
    "    ann_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    i = 1\n",
    "    for ref in ann_list_xml.findAll('markable'):\n",
    "        this = {}\n",
    "\n",
    "        this[\"id\"] = ref['id']\n",
    "        this[\"span\"] = ref['span']\n",
    "        this[\"coref_class\"] = ref['coref_class']\n",
    "        this[\"mmax_level\"] = ref['mmax_level']\n",
    "        this[\"sure\"] = ref['sure']\n",
    "        this[\"np_form\"] = ref['np_form'] \n",
    "        this['first_span'] = ref['span'].split('..')[0] #I am taking the first word of the span, to have one to one\n",
    "        this['doc'] = j\n",
    "        \n",
    "        aclpro_all_ann[j][i] = this\n",
    "        i += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coref_class': 'set_114',\n",
       " 'doc': 1,\n",
       " 'first_span': 'word_1438',\n",
       " 'id': 'markable_825',\n",
       " 'mmax_level': 'coref',\n",
       " 'np_form': 'ne',\n",
       " 'span': 'word_1438..word_1443',\n",
       " 'sure': 'yes'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aclpro_all_ann[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the structure of wikipro\n",
    "# <markable \n",
    "# id=\"markable_318\" \n",
    "# span=\"word_1..word_1\" \n",
    "# coref_class=\"set_47\" \n",
    "# topic=\"http://rdf.freebase.com/ns/m.010kl\" \n",
    "# coreftype=\"ident\" \n",
    "# mentiontype=\"ne\" \n",
    "# mmax_level=\"coref\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipro_all_ann = {}\n",
    "j = 1\n",
    "for file in wikipro_ann_files:\n",
    "    wikipro_all_ann[j] = {}\n",
    "    ann_list_xml = BeautifulSoup(open(file),\"xml\")\n",
    "    i = 1\n",
    "    for ref in ann_list_xml.findAll('markable'):\n",
    "        this = {\"id\":'',\"span\":'',\"coref_class\":'',\"topic\":'',\"coreftype\":'',\"mentiontype\":'',\"mmax_level\":''}\n",
    "        this['doc'] = j\n",
    "        this[\"id\"] = ref['id']\n",
    "        this[\"span\"] = ref['span']\n",
    "        this[\"coref_class\"] = ref['coref_class']\n",
    "        this[\"topic\"] = ref['topic']\n",
    "        this[\"coreftype\"] = ref['coreftype']\n",
    "        this[\"mentiontype\"] = ref['mentiontype']\n",
    "        this[\"mmax_level\"] = ref['mmax_level']        \n",
    "        this['first_span'] = ref['span'].split('..')[0] #I am taking the first word of the span, to have one to one\n",
    "        wikipro_all_ann[j][i] = this\n",
    "        i += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coref_class': 'set_14',\n",
       " 'coreftype': 'ident',\n",
       " 'doc': 9,\n",
       " 'first_span': 'word_93',\n",
       " 'id': 'markable_334',\n",
       " 'mentiontype': 'pro',\n",
       " 'mmax_level': 'coref',\n",
       " 'span': 'word_93..word_93',\n",
       " 'topic': 'nan'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipro_all_ann[9][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the data and dumping into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = {1:wikipro_all_ann,2:wikipro_all_words,3:parcor_all_ann,4:parcor_all_words,5:aclpro_all_ann,6:aclpro_all_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle all of the data that we stored into the all_data dictionary\n",
    "with open('all_data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(all_data, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checking that the dictionary got pickled \n",
    "with open(\"all_data.pkl\", 'rb') as picklefile: \n",
    "    all_data = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
